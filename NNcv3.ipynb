{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import knižníc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Base `Layer` class\n",
    "Each layer in the network should be able to perform:\n",
    "1. **Forward** traversal (prediction)\n",
    "2. **Backward** traversal (gradient computation and weight update)\n",
    "\n",
    "Here we define a generic `Layer` class that other layers will inherit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, inp):\n",
    "     return inp\n",
    "\n",
    "    def backward(self, inp, grad_outp):\n",
    "       return grad_outp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Activation functions\n",
    "We define `ReLU` and `Sigmoid` as special layers inheriting from the `Layer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inp):\n",
    "        output = np.maximum(0,inp)\n",
    "        return output\n",
    "\n",
    "    def backward(self, inp, grad_outp):\n",
    "        bool_mask = inp > 0\n",
    "        return grad_outp * bool_mask\n",
    "\n",
    "\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inp):\n",
    "        output = 1/(1 + np.exp(-inp))\n",
    "        return output\n",
    "\n",
    "    def backward(self, inp, grad_outp):\n",
    "        output  = self.forward(inp) * (1 - self.forward(inp)) * grad_outp\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee15aa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inp):\n",
    "\n",
    "        shifted_inp = inp - np.max(inp, axis=1, keepdims=True)\n",
    "        exps = np.exp(shifted_inp)\n",
    "        output = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        self.output = output  \n",
    "        return output\n",
    "\n",
    "    def backward(self, inp, grad_outp):\n",
    "        \n",
    "        batch_size = inp.shape[0]\n",
    "        grad_inp = np.zeros_like(inp)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            s = self.output[i]  \n",
    "            grad = grad_outp[i]  \n",
    "            s = s.reshape(-1, 1)  \n",
    "            jac = np.diagflat(s) - np.dot(s, s.T)  \n",
    "            grad_inp[i] = np.dot(grad, jac)  \n",
    "        \n",
    "        return grad_inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dense layer\n",
    "This layer has parameters (weights `W` and biases `b`) and performs the calculation of a linear transformation: \\( z = xW + b \\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, inp_units, outp_units, learning_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.lr = learning_rate\n",
    "        self.outp_units = outp_units\n",
    "        #self.W = np.random.randn(inp_units, outp_units) * 0.1\n",
    "        self.W = np.random.randn(inp_units, outp_units) * np.sqrt(2.0 / inp_units)\n",
    "        self.b = np.zeros(outp_units)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        z = np.dot(inp, self.W) + self.b\n",
    "        return z\n",
    "\n",
    "    def backward(self, inp, grad_outp):\n",
    "\n",
    "        dW = np.dot(inp.T, grad_outp)\n",
    "        db = np.sum(grad_outp, axis=0)\n",
    "        grad_inp = np.dot(grad_outp, self.W.T) \n",
    "        self.W -= self.lr *dW\n",
    "        self.b -= self.lr * db\n",
    "        return grad_inp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The MLP class itself\n",
    "Contains several layers (Dense + activation), supports:\n",
    "- `add_layer(...)` to add a new layer\n",
    "- `forward(X)` to pass forward through the entire network\n",
    "- `predict(X)` to predict\n",
    "- `fit(X, y)` to train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self):\n",
    "        self.layers = []  \n",
    "        self.layer_inputs = []\n",
    "\n",
    "    def add_layer(self, neuron_count, inp_shape=None, activation='ReLU', learning_rate=0.1):\n",
    "        inp_units = 0\n",
    "        if inp_shape is not None:\n",
    "            inp_units = inp_shape\n",
    "        else:\n",
    "            last_dense = None\n",
    "            for layer in reversed(self.layers):\n",
    "                if isinstance(layer, Dense):\n",
    "                    last_dense = layer\n",
    "                    break\n",
    "            if last_dense is None:\n",
    "                raise ValueError(\"input shape is not specified\")\n",
    "            inp_units = last_dense.outp_units\n",
    "        \n",
    "        dense_layer = Dense(inp_units=inp_units, outp_units=neuron_count, learning_rate=learning_rate)\n",
    "        self.layers.append(dense_layer)\n",
    "        \n",
    "        if activation == \"ReLU\":\n",
    "            self.layers.append(ReLU())\n",
    "        elif activation == 'Sigmoid':\n",
    "            self.layers.append(Sigmoid())\n",
    "        elif activation == 'Softmax':\n",
    "            self.layers.append(Softmax())\n",
    "        elif activation is None:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation:\", activation)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.layer_inputs = []\n",
    "        activation = X\n",
    "        for layer in self.layers:\n",
    "            self.layer_inputs.append(activation)\n",
    "            activation = layer.forward(activation)\n",
    "        return activation\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "    def fit(self, X, y, epochs=10):\n",
    "        \n",
    "        N = X.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            prediction = self.forward(X) \n",
    "            Loss = (1 / (2 * N)) * np.sum((y - prediction)**2) \n",
    "            \n",
    "            grad_outp = (prediction-y) / N\n",
    "            for i in range(len(self.layers) - 1, -1, -1):\n",
    "                layer = self.layers[i]\n",
    "                inp = self.layer_inputs[i]\n",
    "                grad_outp = layer.backward(inp, grad_outp)\n",
    "            if epoch % 500 == 0:\n",
    "                print(f\"Epoch {epoch} MSE = {Loss}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing part (main)\n",
    "After completing all TODO, it will be possible to create the network, add layers and call the `predict(...)` or `fit(...)` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" if __name__ == \"__main__\":\n",
    "    # Dummy test\n",
    "    X = np.array([[0, 0], [1, 0], [0, 1], [2, 0], [0, 2], [1, 1]])\n",
    "    y = np.array([[1, 0, 0],  \n",
    "                [0, 1, 0],  \n",
    "                [0, 1, 0],  \n",
    "                [0, 0, 1],  \n",
    "                [0, 0, 1],  \n",
    "                [0, 1, 0]])\n",
    "    network = MLP()\n",
    "    network.add_layer(neuron_count=4, inp_shape=2, activation='ReLU')  \n",
    "    network.add_layer(neuron_count=3, activation='Softmax')\n",
    "    network.fit(X, y, epochs=1000)\n",
    "    print(\"Предсказания:\", network.predict(X))\n",
    "\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "- **Fill in** the missing implementation details in each class.\n",
    "- Test (custom cases or add `network.fit(...)`).\n",
    "- Extend as needed (more layers, different activation functions).\n",
    "\n",
    "After successful completion, you should be able to create a network, train it on a small data set, and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b442dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cecd2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"shibumohapatra/house-price\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9495d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls /home/ir739wb/.cache/kagglehub/datasets/shibumohapatra/house-price/versions/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be1c6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path + '/1553768847-housing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14374d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de08f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2e2367",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_bedrooms'] = df['total_bedrooms'].fillna(df['total_bedrooms'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb0525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a3d178",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ocean_proximity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea703c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e = pd.get_dummies(df, columns=['ocean_proximity'], drop_first=True, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5194948",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404da347",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_e.drop('median_house_value', axis=1)\n",
    "y = df_e['median_house_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728d7438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=42, test_size=0.25,shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840a6868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a621ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = x_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "#x_test = x_test.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268d9101",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae6ba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_train.reshape(15480,1)\n",
    "y_test = y_test.reshape(5160,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c88e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f88b088",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "network = MLP()\n",
    "network.add_layer(neuron_count=12, inp_shape=12, activation='ReLU', learning_rate=0.001)  \n",
    "network.add_layer(neuron_count=10, activation='ReLU', learning_rate=0.001)  \n",
    "network.add_layer(neuron_count=8, activation='ReLU', learning_rate=0.001)  \n",
    "network.add_layer(neuron_count=6, activation='ReLU', learning_rate=0.001)  \n",
    "network.add_layer(neuron_count=1, activation=None, learning_rate=0.0001)\n",
    "network.fit(x_train, y_train, epochs=60000)\n",
    "print(\"prediction:\", network.predict(x_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14576b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
