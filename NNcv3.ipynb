{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import knižníc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Base `Layer` class\n",
    "Each layer in the network should be able to perform:\n",
    "1. **Forward** traversal (prediction)\n",
    "2. **Backward** traversal (gradient computation and weight update)\n",
    "\n",
    "Here we define a generic `Layer` class that other layers will inherit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, inp):\n",
    "     return inp\n",
    "\n",
    "    def backward(self, inp, grad_outp):\n",
    "       return grad_outp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Activation functions\n",
    "We define `ReLU` and `Sigmoid` as special layers inheriting from the `Layer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inp):\n",
    "        output = np.maximum(0,inp)\n",
    "        return output\n",
    "\n",
    "    def backward(self, inp, grad_outp):\n",
    "        bool_mask = inp > 0\n",
    "        return grad_outp * bool_mask\n",
    "\n",
    "\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inp):\n",
    "        output = 1/(1 + np.exp(-inp))\n",
    "        return output\n",
    "\n",
    "    def backward(self, inp, grad_outp):\n",
    "        output  = self.forward(inp) * (1 - self.forward(inp)) * grad_outp\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee15aa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inp):\n",
    "\n",
    "        shifted_inp = inp - np.max(inp, axis=1, keepdims=True)\n",
    "        exps = np.exp(shifted_inp)\n",
    "        output = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        self.output = output  \n",
    "        return output\n",
    "\n",
    "    def backward(self, inp, grad_outp):\n",
    "        \n",
    "        batch_size = inp.shape[0]\n",
    "        grad_inp = np.zeros_like(inp)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            s = self.output[i]  \n",
    "            grad = grad_outp[i]  \n",
    "            s = s.reshape(-1, 1)  \n",
    "            jac = np.diagflat(s) - np.dot(s, s.T)  \n",
    "            grad_inp[i] = np.dot(grad, jac)  \n",
    "        \n",
    "        return grad_inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dense layer\n",
    "This layer has parameters (weights `W` and biases `b`) and performs the calculation of a linear transformation: \\( z = xW + b \\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, inp_units, outp_units, learning_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.lr = learning_rate\n",
    "        self.outp_units = outp_units\n",
    "        self.W = np.random.randn(inp_units, outp_units) * 0.1\n",
    "        self.b = np.zeros(outp_units)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        z = np.dot(inp, self.W) + self.b\n",
    "        return z\n",
    "\n",
    "    def backward(self, inp, grad_outp):\n",
    "\n",
    "        dW = np.dot(inp.T, grad_outp)\n",
    "        db = np.sum(grad_outp, axis=0)\n",
    "        grad_inp = np.dot(grad_outp, self.W.T) \n",
    "        self.W -= self.lr *dW\n",
    "        self.b -= self.lr * db\n",
    "        return grad_inp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The MLP class itself\n",
    "Contains several layers (Dense + activation), supports:\n",
    "- `add_layer(...)` to add a new layer\n",
    "- `forward(X)` to pass forward through the entire network\n",
    "- `predict(X)` to predict\n",
    "- `fit(X, y)` to train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self):\n",
    "        self.layers = []  \n",
    "        self.layer_inputs = []\n",
    "\n",
    "    def add_layer(self, neuron_count, inp_shape=None, activation='ReLU', learning_rate=0.1):\n",
    "        inp_units = 0\n",
    "        if inp_shape is not None:\n",
    "            inp_units = inp_shape\n",
    "        else:\n",
    "            last_dense = None\n",
    "            for layer in reversed(self.layers):\n",
    "                if isinstance(layer, Dense):\n",
    "                    last_dense = layer\n",
    "                    break\n",
    "            if last_dense is None:\n",
    "                raise ValueError(\"input shape is not specified\")\n",
    "            inp_units = last_dense.outp_units\n",
    "        \n",
    "        dense_layer = Dense(inp_units=inp_units, outp_units=neuron_count, learning_rate=learning_rate)\n",
    "        self.layers.append(dense_layer)\n",
    "        \n",
    "        if activation == \"ReLU\":\n",
    "            self.layers.append(ReLU())\n",
    "        elif activation == 'Sigmoid':\n",
    "            self.layers.append(Sigmoid())\n",
    "        elif activation == 'Softmax':\n",
    "            self.layers.append(Softmax())\n",
    "        elif activation is None:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation:\", activation)\n",
    "        \n",
    "    def forward(self, X):\n",
    "\n",
    "        activation = X\n",
    "        for layer in self.layers:\n",
    "            self.layer_inputs.append(activation)\n",
    "            activation = layer.forward(activation)\n",
    "        return activation\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "    def fit(self, X, y, epochs=10):\n",
    "        \n",
    "        N = X.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            prediction = self.forward(X) \n",
    "            Loss = (1 / (2 * N)) * np.sum((y - prediction)**2) \n",
    "            \n",
    "            grad_outp = (y-prediction) / N\n",
    "            for i in range(len(self.layers) - 1, -1, -1):\n",
    "                layer = self.layers[i]\n",
    "                inp = self.layer_inputs[i]\n",
    "                grad_outp = layer.backward(inp, grad_outp)\n",
    "            print(f\"Epoch {epoch} MSE = {Loss}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing part (main)\n",
    "After completing all TODO, it will be possible to create the network, add layers and call the `predict(...)` or `fit(...)` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Dummy test\n",
    "    X = np.array([[0, 0], [1, 0], [0, 1], [2, 0], [0, 2], [1, 1]])\n",
    "    y = np.array([[1, 0, 0],  \n",
    "                [0, 1, 0],  \n",
    "                [0, 1, 0],  \n",
    "                [0, 0, 1],  \n",
    "                [0, 0, 1],  \n",
    "                [0, 1, 0]])\n",
    "    network = MLP()\n",
    "    network.add_layer(neuron_count=4, inp_shape=2, activation='ReLU')  \n",
    "    network.add_layer(neuron_count=3, activation='Softmax')\n",
    "    network.fit(X, y, epochs=1000)\n",
    "    print(\"Предсказания:\", network.predict(X))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "- **Fill in** the missing implementation details in each class.\n",
    "- Test (custom cases or add `network.fit(...)`).\n",
    "- Extend as needed (more layers, different activation functions).\n",
    "\n",
    "After successful completion, you should be able to create a network, train it on a small data set, and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b442dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cecd2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "df = pd.DataFrame(iris.data, columns = iris.feature_names)\n",
    "df['target'] = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a9e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fdbe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b073b7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c783d9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c054c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57432bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[10:21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5611fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_df = df[[\"sepal length (cm)\", 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b843218",
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab73c025",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7b0f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['target'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caf12b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50bfacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_df = df[\"sepal length (cm)\"].sum()\n",
    "mean_df = df[\"sepal length (cm)\"].mean()\n",
    "median_df = df[\"sepal length (cm)\"].median()\n",
    "\n",
    "print(f\"Sum: {sum_df}, \\nmean: {mean_df}, \\nMedian: {median_df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6640b24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = df[\"sepal length (cm)\"].min()\n",
    "max_df = df[\"sepal length (cm)\"].max()\n",
    "\n",
    "print(f\"Minimum:{min_df}, maximum: {max_df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258f4bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1b7a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = cols[0:4]\n",
    "df1  = df[cols]\n",
    "df[\"total values\"] = df1.sum(axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed5bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "newcols={\n",
    "    \"sepal length (cm)\" : \"sepallength\",\n",
    "    \"sepal width (cm)\" : \"sepalwidth\",\n",
    "    \"petal length (cm)\" : \"petallength\",\n",
    "    \"petal width (cm)\" : \"petalwidth\"\n",
    "}\n",
    "df.rename(columns=newcols, inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4bd907",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a864b5d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepallength     0\n",
       "sepalwidth      0\n",
       "petallength     0\n",
       "petalwidth      0\n",
       "target          0\n",
       "total values    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06c7979a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "lmplot() got multiple values for argument 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43msns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlmplot\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msepallength\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msepalwidth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfit_reg\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: lmplot() got multiple values for argument 'data'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.lmplot('sepallength', 'sepalwidth', data=df,fit_reg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9495d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
